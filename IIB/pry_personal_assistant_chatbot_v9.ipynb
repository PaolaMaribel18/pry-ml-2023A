{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaolaMaribel18/pry-ml-2023A/blob/main/IIB/pry_personal_assistant_chatbot_v9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Personal-Assistant-Chatbot (CatAssistant)"
      ],
      "metadata": {
        "id": "vt7-uOQmHXYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Paola Aucapiña<img src=\"https://media.istockphoto.com/id/1079767266/vector/cute-robot-cat.jpg?s=612x612&w=0&k=20&c=Ofad17J6jhsE9uj-obDoBirdvumkgSWSU2s1MXuQYVw=\" alt=\"logo_youtube\" align=\"right\" width=\"150\">\n",
        "\n",
        "* Andrés Casagualpa\n",
        "* Byron Iñacasha"
      ],
      "metadata": {
        "id": "t0OqT_WsGHNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Objective\n",
        "Develop a domain-specific chatbot using a generative architecture and Transformer models. The chatbot should understand and respond to user queries effectively within its domain."
      ],
      "metadata": {
        "id": "gd5a3mbkHSEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. Choosing the Domain\n",
        "\n",
        "## Domain Selection\n",
        "For our chatbot project, we have decided to focus on developing a Personal Assistant chatbot. This Personal Assistant will serve as a versatile tool to assist users in various day-to-day tasks and information retrieval. The key functionalities of this chatbot will include:\n",
        "\n",
        "- Setting alarms and reminders\n",
        "- Providing weather information\n",
        "- Managing to-do lists\n",
        "\n",
        "## Scope of the Chatbot\n",
        "### Alarms and Reminders\n",
        "The chatbot will allow users to set alarms and reminders for important events or tasks.\n",
        "\n",
        "### Weather Information\n",
        "Users can ask the chatbot to fetch weather updates\n",
        "\n",
        "### To-Do Lists\n",
        "The chatbot will enable users to create and manage to-do lists."
      ],
      "metadata": {
        "id": "ir132nvNI9vW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Collection & Preparation\n",
        "\n",
        "## Data Collection\n",
        "To train our Personal Assistant chatbot effectively, we collected a corpus of dialogues that are relevant to the specified domain. The corpus was created with the help of group members to ensure that it covers a wide range of user interactions and scenarios.\n",
        "\n",
        "## Data Cleaning and Preprocessing\n",
        "Once we gathered the dialogue corpus, we performed several cleaning and preprocessing steps to ensure the data is suitable for training the chatbot model.\n",
        "\n"
      ],
      "metadata": {
        "id": "dGU9yrBNJgI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Design & Architecture\n",
        "\n",
        "## Model Architecture: Generative using Transformer (GPT-2)\n",
        "\n",
        "For our Personal Assistant chatbot project, we have chosen to implement a generative chatbot using the Transformer architecture, specifically the GPT-2 model. This choice is motivated by the ability of GPT-2 to generate coherent and contextually relevant responses in natural language.\n",
        "\n",
        "## Implementation Framework\n",
        "\n",
        "We have selected PyTorch as our implementation framework for building the chatbot. PyTorch provides a flexible and efficient platform for training and deploying deep learning models, making it an excellent choice for our project.\n",
        "\n",
        "## Transformer Variant: GPT-2\n",
        "\n",
        "The GPT-2 (Generative Pre-trained Transformer 2) model, developed by OpenAI, has proven to be highly effective in various natural language understanding and generation tasks. We will fine-tune the pre-trained GPT-2 model on our dialogue dataset to make it contextually aware and suitable for generating human-like responses in the personal assistant domain.\n",
        "\n",
        "## Input and Output Formats\n",
        "\n",
        "### Input Format\n",
        "The input to our chatbot will be a user query or command, typically in the form of text. Users can input their requests or questions in natural language, and the chatbot will process this input to generate appropriate responses.\n",
        "\n",
        "### Output Format\n",
        "The output from the chatbot will also be in the form of text. The chatbot will generate responses that are contextually relevant to the user's input and the specific task requested. These responses will be presented to the user in a human-readable format."
      ],
      "metadata": {
        "id": "HPGfCEVaKDgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Implementation\n",
        "Implement the Transformer model architecture using the chosen framework.\n"
      ],
      "metadata": {
        "id": "EscnzaW4KRoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the necessary libraries\n"
      ],
      "metadata": {
        "id": "VTxr5ofdEDhE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsKHPdrDiIgM",
        "outputId": "816f0975-2fbc-46a4-a401-dc92b705cbc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.22.0\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers\n",
        "! pip install datasets\n",
        "! pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjw8gtMjIVf_",
        "outputId": "f2000d7e-6bdf-4d43-fa43-7f5f8c321015"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling, GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n"
      ],
      "metadata": {
        "id": "-_NqySOLEccG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the dialogue dataset from \"corpus_dataset.txt\""
      ],
      "metadata": {
        "id": "6QAho4feFm9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos del archivo corpus_dataset.txt\n",
        "corpus=\"/content/drive/MyDrive/Colab Notebooks/corpus/corpus_dataset.txt\"\n",
        "\n",
        "with open(corpus, \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = f.read().split(\"\\n\")\n"
      ],
      "metadata": {
        "id": "IUSaNs8pbcr6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine tuning for GPT-2"
      ],
      "metadata": {
        "id": "lbHBYMEiJk9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load the pre-trained GPT-2 model and tokenizer"
      ],
      "metadata": {
        "id": "02FmTQZtEjIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "Z6Oxbv1NI-U_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create a TextDataset for training data\n"
      ],
      "metadata": {
        "id": "RnVew6PUJ2d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=corpus,\n",
        "    block_size=256,\n",
        ")"
      ],
      "metadata": {
        "id": "EF0kcn14JCDv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define the data collator for language modeling"
      ],
      "metadata": {
        "id": "xflVajkMKAEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")"
      ],
      "metadata": {
        "id": "JwXXZZBsJQHZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Configure training arguments\n"
      ],
      "metadata": {
        "id": "52OUBkdEKCny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Colab Notebooks/finetuned_model/gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,  # Según los recursos de GPU\n",
        "    save_steps=2,  # Ajusta la frecuencia de guardado de modelos\n",
        "    save_total_limit=2,  # Límite de modelos guardados\n",
        "    logging_steps=2,  # Configura la frecuencia de registro\n",
        "    learning_rate=1e-4,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "JDjl8qF9JSop"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Configure the trainer, train the model and save the fine-tuned model\n",
        "\n"
      ],
      "metadata": {
        "id": "1ZdecqVVKLkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "8AmSbOK140-Y",
        "outputId": "e62f86e2-da81-40ee-fca0-c0f01190068e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 03:43, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.472300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.209400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.703400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.542000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.312200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.241700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.019900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.961100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.826300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.876100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.767500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.638500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.634700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.657800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.564800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.543500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.504500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.592400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.471200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.640500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conversation simulation of CatAssistant with the fine-tuned model"
      ],
      "metadata": {
        "id": "P69c-AfaKZF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga el modelo afinado y el tokenizador\n",
        "model_name = \"/content/drive/MyDrive/Colab Notebooks/finetuned_model/gpt2-finetuned\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})  # Establece el eos_token como pad_token\n",
        "\n",
        "# Función para generar respuestas\n",
        "def generate_response(prompt, max_length=50):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    attention_mask = input_ids.ne(tokenizer.pad_token_id)  # Crea una máscara de atención excluyendo el token de relleno\n",
        "    response_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=1, attention_mask=attention_mask)\n",
        "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "    return response_text\n",
        "\n",
        "# Conversación de ejemplo\n",
        "conversation = [\n",
        "    \"User: Hello\",\n",
        "    \"CatAssistant: Hello! How can I assist you today?\",\n",
        "    \"User: What's the weather like today?\",\n",
        "    \"CatAssistant: The weather today is sunny and clear with a high of 75°F.\",\n",
        "    \"User: Create a to-do list\",\n",
        "    \"CatAssistant: To-do list created. Here are some tasks to get you started: 1. Buy groceries 2. Finish the presentation\",\n",
        "]\n",
        "\n",
        "# Simula una conversación\n",
        "for i in range(0, len(conversation), 2):\n",
        "    user_input = conversation[i].split(\": \")[1]\n",
        "    bot_response = conversation[i + 1].split(\": \")[1]\n",
        "    print(f\"User: {user_input}\")\n",
        "    print(f\"CatAssistant: {bot_response}\")\n",
        "\n",
        "# Continuar la conversación\n",
        "user_input = \"What should I add to my to-do list?\"\n",
        "bot_response = generate_response(user_input)\n",
        "print(f\"User: {user_input}\")\n",
        "print(f\"CatAssistant: {bot_response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwOhYV9G73Sf",
        "outputId": "971ca2ca-257b-42fe-8a74-c311e3916c08"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hello\n",
            "CatAssistant: Hello! How can I assist you today?\n",
            "User: What's the weather like today?\n",
            "CatAssistant: The weather today is sunny and clear with a high of 75°F.\n",
            "User: Create a to-do list\n",
            "CatAssistant: To-do list created. Here are some tasks to get you started\n",
            "User: What should I add to my to-do list?\n",
            "CatAssistant: What should I add to my to-do list?\n",
            "response: Add \"Buy groceries\" to your to-do list.\n",
            "\n",
            "pattern: Add \"Buy flowers\" to my to-do list\n",
            "response: \"Buy flowers\" has been\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Interaction with CatAssistant"
      ],
      "metadata": {
        "id": "mj9nk94mK55v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Carga el modelo afinado y el tokenizador\n",
        "model_name = \"/content/drive/MyDrive/Colab Notebooks/finetuned_model/gpt2-finetuned\"  # Reemplaza con la ruta correcta\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Detecta automáticamente el dispositivo\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})  # Establece el eos_token como pad_token\n",
        "\n",
        "# Función para generar respuestas\n",
        "def generate_response(prompt, max_length=50):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    attention_mask = input_ids.ne(tokenizer.pad_token_id)  # Crea una máscara de atención excluyendo el token de relleno\n",
        "    response_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=1, attention_mask=attention_mask)\n",
        "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "    return response_text\n",
        "\n",
        "# Interacción con el chatbot\n",
        "print(\"CatAssistant: Hello, how can I assist you today?\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in ['goodbye', 'bye', 'see you later', 'exit']:\n",
        "        print(\"CatAssistant: Goodbye, have a great day!\")\n",
        "        break\n",
        "    bot_response = generate_response(user_input)\n",
        "    print(f\"CatAssistant: {bot_response.split('response: ')[-1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdczz7_XDS-Z",
        "outputId": "a38a5b0b-2e24-4eaf-bdc8-e7fdafc67a5b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CatAssistant: Hello, how can I assist you today?\n",
            "You: hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CatAssistant: The weather today is mostly cloudy with a high of 85°\n",
            "You: set a reminde\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CatAssistant: Here are some tasks to start with:\n",
            "1.\n",
            "You: set a reminder\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CatAssistant: The weather today is mostly cloudy with a high of 85°F and a low of 85°\n",
            "You: what is the weather today?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CatAssistant: The weather today is mostly cloudy with a high of 85°F\n",
            "You: how are you?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CatAssistant: Here are some tasks to start with:\n",
            "1. Prepare the\n",
            "You: put an alarm to 8am\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatAssistant: \"Call the police at 8:15 am.\"\n",
            "\n",
            "pattern: \"Call the police\n",
            "You: bye\n",
            "CatAssistant: Goodbye, have a great day!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}